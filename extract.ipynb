{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aacab6a-f3ac-4e18-b1d8-536f9d94be6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import urllib\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import http.cookiejar\n",
    "import docx\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import tldextract\n",
    "import traceback\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49b81ba1-7c51-4179-8047-dbd3977d0f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as util\n",
    "import config_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f67cd4e4-5e03-4e6c-92b5-e716ad2b16b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "serverConfig = config_manager.get_server_config()\n",
    "storage = serverConfig['storage']\n",
    "bucket_name = serverConfig['default_bucket']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c386807b-e550-4457-ac33-42f381d488b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_soup(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the URL with headers\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content using BeautifulSoup\n",
    "            page_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            return page_soup\n",
    "        else:\n",
    "            print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f99cc77-6532-4a8b-9bbf-497de4287fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.esgtoday.com/climeworks-announces-direct-air-capture-technology-breakthrough-to-scale-carbon-removal/\"\n",
    "page_soup = get_page_soup(url)\n",
    "website = \"esgtoday\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c54f4b4-c61a-47f8-850d-a77d04d36249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractTitleAuthorDateContent(page_soup,articles_config,website):\n",
    "    '''\n",
    "    Using html tags from an article webpage, extracting title, author,published date and context from the articles\n",
    "    '''\n",
    "    response_obj = {\"status\": \"success\", \"message\": \"\", \"file_path\": \"\",\"file_size_in_kb\":\"\"}\n",
    "    unwanted_class_names = [\"author-bio__author-card__3h9uA\",\"site-footer__content-container__3ASGG\",\"media-story-card__body__3tRWy\",\"mb-20 stryalsoread\",\"text__text__1FZLe text__dark-grey__3Ml43 text__regular__2N1Xr text__ultra_small__37j9j body__base__22dCE body__ultra_small_body__1lUQl primary-gallery__caption__1UdH8\",\"caption\",\"attribution\"]\n",
    "    unwanted_tags = page_soup.find_all(class_=unwanted_class_names)\n",
    "    for tag in unwanted_tags:\n",
    "        tag.decompose()\n",
    "    \n",
    "    title = page_soup.findAll(articles_config[website]['title']['tag_name'], class_=articles_config[website]['title']['class_name']) if website == 'esgclarity' else page_soup.findAll(articles_config[website]['title']['tag_name'], class_=articles_config[website]['title']['class_name'])\n",
    "    if website==\"business-standard\":\n",
    "        extra_content= page_soup.findAll(articles_config[website]['extra']['tag_name'], class_=articles_config[website]['extra']['class_name'])\n",
    "#         print(extra_content)\n",
    "        extra_info = \" \".join([element.get_text() for element in extra_content])\n",
    "    else:\n",
    "        extra_info=title[0].text.strip().split('\\n')[-1].strip()\n",
    "    title = title[0].text.split('\\n')[1] if title != [] and website == \"esgclarity\" else title[0].text.strip() if title else \"\"\n",
    "    print(\"Title : \", title)\n",
    "    \n",
    "    # Extracting Publisher name of the Article from the website\n",
    "    if website == \"theconversation\":\n",
    "        author_element = [a.findAll(articles_config[website][\"author\"][\"tag_name_1\"], class_=articles_config[website][\"author\"][\"class_name_1\"]) for a in page_soup.findAll(articles_config[website][\"author\"][\"tag_name_2\"],class_=articles_config[website][\"author\"][\"class_name_2\"]) if a.findAll(articles_config[website][\"author\"][\"tag_name_3\"])[0].text in articles_config[website][\"author\"][\"text_list\"]]\n",
    "        author = \", \".join(name.text.strip() for name in author_element[0]) if author_element else \"\"\n",
    "    elif website == \"business-standard\":\n",
    "        author_element = [name.split(\"|\")[0].strip() for sublist in page_soup.findAll(articles_config[website]['author']['tag_name'], class_= articles_config[website]['author']['class_name']) for name in sublist]\n",
    "        author = \", \".join(name for name in author_element) if author_element else \"\"\n",
    "    else:\n",
    "        author_element = [name.text.strip() for name in page_soup.findAll(articles_config[website]['author']['tag_name'], class_=articles_config[website]['author']['class_name'])]\n",
    "        author = \", \".join(name for name in author_element) if author_element else \"\"\n",
    "    print(\"Author : \", author)\n",
    "    # Extracting Published_date and contents of the Article from the websit\n",
    "    if website == \"reuters\":\n",
    "        date = page_soup.findAll(articles_config[website]['date']['tag_name'], class_=articles_config[website]['date']['class_name'])\n",
    "        if date != []:\n",
    "            date = date[0].text\n",
    "        else:\n",
    "            date = \"\"\n",
    "#         paragraphs = [p.get_text(strip=True) for p in page_soup.findAll(\"p\")]\n",
    "        paragraphs = []\n",
    "        for p in page_soup.findAll(\"p\"):  \n",
    "            if 'label' in str(p) or 'class' in p.attrs or 'style' in p.attrs or any(span_tag in p.descendants for span_tag in p.find_all('span')):\n",
    "                text = ''\n",
    "                for element in p.contents:\n",
    "#                     print(element)\n",
    "                    if isinstance(element, str):\n",
    "                        text += element\n",
    "                    elif element.name == 'a':\n",
    "                        text += element.text\n",
    "                if text:\n",
    "                    paragraphs.append(text.strip())\n",
    "        last_two_sentences = paragraphs[-2:]\n",
    "        # print('last_two_sentences',last_two_sentences)\n",
    "        if any(\"reporting by\" in sentence.lower() for sentence in last_two_sentences):\n",
    "            paragraphs = paragraphs[:-2]  # Remove last two sentences\n",
    "        else:\n",
    "            paragraphs = paragraphs[:-1]\n",
    "    else:\n",
    "        date_element = page_soup.find(articles_config[website]['date']['tag_name'], class_=articles_config[website]['date']['class_name'])\n",
    "        if date_element != None:\n",
    "            date_text_old = date_element.text\n",
    "            regEx = r'(?:\\d{1,2}[-/th|st|nd|rd\\s]*)?(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z\\s,.]*(?:\\d{1,2}[-/th|st|nd|rd)\\s,]*)?(?:\\d{2,4})'\n",
    "            date_text_l = re.findall(regEx,date_text_old )\n",
    "            date_text = date_text_l[0]\n",
    "            date = date_text\n",
    "        else:\n",
    "            date = \"\"\n",
    "\n",
    "        class_names = articles_config[website]['context'].get('class_names')\n",
    "        if class_names:\n",
    "            paragraphs = []\n",
    "            strings_to_remove = [\"Related Article:\", \"See related article:\",\"To learn more, visit\"]\n",
    "            for class_name in class_names:\n",
    "                paragraph_elements = page_soup.findAll(articles_config[website]['context']['tag_name'], class_=class_name)\n",
    "                for p in paragraph_elements:\n",
    "                    a_tag_target = p.find('a', href=True, target=\"_blank\", rel=\"noreferrer noopener\")\n",
    "                    if a_tag_target:\n",
    "                        a_tag_target.extract()\n",
    "                        for string_to_remove in strings_to_remove:\n",
    "                            p.string = p.get_text().replace(string_to_remove, \"\").strip()\n",
    "                    em_tags = p.find_all('em')\n",
    "                    # print(em_tags,\"em_tag\")\n",
    "                    a_tag = p.find('a', href=True)\n",
    "                    for em_tag in em_tags:\n",
    "                        if a_tag and em_tag:\n",
    "                            em_tag.extract()\n",
    "                            a_tag.extract()\n",
    "                    \n",
    "                    strong_tags = p.find_all('strong')\n",
    "                    # print('strong_tags',strong_tags,'strong_tags')\n",
    "                    for strong_tag in strong_tags:\n",
    "                        a_tag_inside_strong = strong_tag.find('a', href=True)\n",
    "                        if a_tag_inside_strong:\n",
    "                            strong_tag.extract()\n",
    "                    \n",
    "                    paragraphs.append(p.get_text().strip())\n",
    "                # paragraphs.extend([p.text.strip() for p in paragraph_elements])\n",
    "        else:\n",
    "            paragraphs = []\n",
    "        paragraphs = ' '.join(paragraphs)\n",
    "        if title!=extra_info:\n",
    "            paragraphs=extra_info+'\\n'+paragraphs\n",
    "        paragraphs = re.split('\\n+', paragraphs)\n",
    "    if paragraphs and (\"source:\" in paragraphs[-1].lower() or \"esg clarity’fund\" in paragraphs[-1].lower()):\n",
    "        paragraphs = paragraphs[:-1]\n",
    "\n",
    "    print(\"Date : \", date)    \n",
    "    if len(paragraphs) >= 1:\n",
    "        if paragraphs[0] != \"\":       \n",
    "            title = title + \".\" if title and not title.endswith(\".\") else title\n",
    "            author = author + \".\" if author and not author.endswith(\".\") else author\n",
    "            date = date + \".\" if date and not date.endswith(\".\") else date\n",
    "            if title:\n",
    "                response_obj['title']=title\n",
    "            else:\n",
    "                response_obj['title']=\"\"\n",
    "                response_obj[\"message\"] += \"Title is empty.\"\n",
    "                print(articles_config[website]['context']['class_name'],'title tag is not found')\n",
    "            if author:\n",
    "                response_obj['author'] = author\n",
    "            else:\n",
    "                response_obj['author'] =\"\"\n",
    "                response_obj[\"message\"] += \"author is empty.\" \n",
    "                print(articles_config[website]['context']['class_name'],'title tag is not found')\n",
    "            if date:\n",
    "                response_obj['date']= date\n",
    "            else:\n",
    "                response_obj['date']= \"\"\n",
    "                response_obj[\"message\"] += \"date is empty.\"\n",
    "                print(articles_config[website]['context']['class_name'],'date tag is not found')\n",
    "            return response_obj,paragraphs\n",
    "\n",
    "        else:\n",
    "            response_obj[\"status\"] = \"error\"\n",
    "            response_obj[\"message\"] = \"No data found.\"\n",
    "            print(articles_config[website]['context']['class_names'],'context tag is not found')\n",
    "#             print(response_obj)\n",
    "            return response_obj,\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "522ee9cf-7a4b-4807-abb6-acc77f2bda4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f = open('articles_config.json')\n",
    "articles_config = json.load(f)\n",
    "user_id=0\n",
    "user_type='private'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac41560b-e5a5-4c96-bd0a-70cc88242616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title :  Climeworks Announces Direct Air Capture Technology Breakthrough to Scale Carbon Removal\n",
      "Author :  Susan Lahey\n",
      "Date :  June 5, 2024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'status': 'success',\n",
       "  'message': '',\n",
       "  'file_path': '',\n",
       "  'file_size_in_kb': '',\n",
       "  'title': 'Climeworks Announces Direct Air Capture Technology Breakthrough to Scale Carbon Removal.',\n",
       "  'author': 'Susan Lahey.',\n",
       "  'date': 'June 5, 2024.'},\n",
       " ['Zurich-based carbon removal company Climeworks revealed today its Generation 3 direct air capture (DAC) technology, providing “breakthrough” improvements in efficiency and performance, and forming a step in the company’s plan to scale up to megaton carbon removal capacity.',\n",
       "  'DAC technology, listed by the IEA as a key carbon removal option in the transition to a net-zero energy system, extracts CO2 directly from the atmosphere for use as a raw material or permanent removal when combined with storage. According to the landmark 2022 Intergovernmental Panel on Climate Change (IPCC) climate change mitigation study, scenarios that limit warming to 1.5°C include carbon dioxide removal methods scaling to billions of tons of removal annually over the coming decades, with DAC positioned to potentially account for a significant portion of the total.',\n",
       "  'Founded in 2009 by Christoph Gebald and Jan Wurzbacher, Climeworks has focused to date on building its position as a leading DAC provider. In 2022, the company\\xa0raised nearly $650 million\\xa0in an equity funding round aimed at scaling its DAC capacity. Last month, Climeworks announced the start-up of “Mammoth,” the largest direct air capture plant in the world to date, with anticipated capability to remove up to 36,000 tons of CO2 per year from the atmosphere, and the company is participating in the development of three megaton hubs in the U.S. that were\\xa0selected by the US Department of Energy (DOE)\\xa0for public funding of more than $600 million.',\n",
       "  'The new Generation 3 technology uses novel structured sorbent materials replacing the packed filter beds used in previous technology generations. The new structures increase surface contact with CO2, reducing the time to capture the CO2 from the air, and substantially increasing the release of CO2 for storage when heated.',\n",
       "  'According to Climeworks, the new solution doubles CO2 capture capacity per module, halves energy consumption, and increases material lifetime, cutting costs by 50%.',\n",
       "  'Climeworks said that it aims to halve overall costs by 2030, and aims to achieve $250-to-$350 per ton captured and total costs of $400-to-$600 per ton net removal.',\n",
       "  'Generation 3 technology was implemented at full scale for the first time in June 2024 at Climeworks’ largest direct air capture testing facility in Switzerland. It will first be deployed in Louisiana as part of the megaton-scale Project Cypress DAC Hub funded by the U.S. Department of Energy, with construction set to begin in 2026.',\n",
       "  'Jan Wurzbacher, Co-founder and Co-CEO said:',\n",
       "  '“Climeworks has always been committed to technology leadership. We were pioneers in the development of direct air capture technology and launched our first commercial facility in 2017. We operate the world’s largest commercial direct air capture plant, Orca, in Iceland, and have inaugurated a larger plant, Mammoth, ten times the size. In parallel, we have, over the past five years, been developing our Generation 3 technology. This development is based on real field data, enabling the scale-up to megaton removal capacities.”        '])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extractTitleAuthorDateContent(page_soup, articles_config, website):\n",
    "    '''\n",
    "    Using html tags from an article webpage, extracting title, author, published date and content from the articles.\n",
    "    '''\n",
    "    response_obj = {\"status\": \"success\", \"message\": \"\", \"file_path\": \"\", \"file_size_in_kb\": \"\"}\n",
    "    unwanted_class_names = [\"footer-headline\",\"source_link picCaption\",\"author-bio__author-card__3h9uA\", \"site-footer__content-container__3ASGG\", \"media-story-card__body__3tRWy\", \"mb-20 stryalsoread\", \"text__text__1FZLe text__dark-grey__3Ml43 text__regular__2N1Xr text__ultra_small__37j9j body__base__22dCE body__ultra_small_body__1lUQl primary-gallery__caption__1UdH8\", \"caption\", \"attribution\", \"data-io-article-url\"]\n",
    "    unwanted_tags = page_soup.find_all(class_=unwanted_class_names)\n",
    "    for tag in unwanted_tags:\n",
    "        tag.decompose()\n",
    "\n",
    "    title = page_soup.findAll(articles_config[website]['title']['tag_name'], class_=articles_config[website]['title']['class_name']) if website == 'esgclarity' else page_soup.findAll(articles_config[website]['title']['tag_name'], class_=articles_config[website]['title']['class_name'])\n",
    "    if website == \"business-standard\":\n",
    "        extra_content = page_soup.findAll(articles_config[website]['extra']['tag_name'], class_=articles_config[website]['extra']['class_name'])\n",
    "        extra_info = \" \".join([element.get_text() for element in extra_content])\n",
    "    else:\n",
    "        extra_info = title[0].text.strip().split('\\n')[-1].strip() if title else \"\"\n",
    "    title = title[0].text.split('\\n')[1] if title != [] and website == \"esgclarity\" else title[0].text.strip() if title else \"\"\n",
    "    print(\"Title : \", title)\n",
    "\n",
    "    # Extracting Publisher name of the Article from the website\n",
    "    if website == \"theconversation\":\n",
    "        author_element = [a.findAll(articles_config[website][\"author\"][\"tag_name_1\"], class_=articles_config[website][\"author\"][\"class_name_1\"]) for a in page_soup.findAll(articles_config[website][\"author\"][\"tag_name_2\"], class_=articles_config[website][\"author\"][\"class_name_2\"]) if a.findAll(articles_config[website][\"author\"][\"tag_name_3\"])[0].text in articles_config[website][\"author\"][\"text_list\"]]\n",
    "        author = \", \".join(name.text.strip() for name in author_element[0]) if author_element else \"\"\n",
    "    elif website == \"business-standard\":\n",
    "        author_element = [name.split(\"|\")[0].strip() for sublist in page_soup.findAll(articles_config[website]['author']['tag_name'], class_=articles_config[website]['author']['class_name']) for name in sublist]\n",
    "        author = \", \".join(name for name in author_element) if author_element else \"\"\n",
    "    else:\n",
    "        author_element = [name.text.strip() for name in page_soup.findAll(articles_config[website]['author']['tag_name'], class_=articles_config[website]['author']['class_name'])]\n",
    "        author = \", \".join(name for name in author_element) if author_element else \"\"\n",
    "    print(\"Author : \", author)\n",
    "\n",
    "    # Extracting Published_date and contents of the Article from the website\n",
    "    if website == \"reuters\":\n",
    "        date = page_soup.findAll(articles_config[website]['date']['tag_name'], class_=articles_config[website]['date']['class_name'])\n",
    "        if date != []:\n",
    "            date = date[0].text\n",
    "        else:\n",
    "            date = \"\"\n",
    "        paragraphs = []\n",
    "        for p in page_soup.findAll(\"p\"):\n",
    "            if 'label' in str(p) or 'class' in p.attrs or 'style' in p.attrs or any(span_tag in p.descendants for span_tag in p.find_all('span')):\n",
    "                text = ''\n",
    "                for element in p.contents:\n",
    "                    if isinstance(element, str):\n",
    "                        text += element\n",
    "                    elif element.name == 'a':\n",
    "                        text += element.text\n",
    "                if text:\n",
    "                    paragraphs.append(text.strip())\n",
    "        last_two_sentences = paragraphs[-2:]\n",
    "        if any(\"reporting by\" in sentence.lower() for sentence in last_two_sentences):\n",
    "            paragraphs = paragraphs[:-2]\n",
    "        else:\n",
    "            paragraphs = paragraphs[:-1]\n",
    "    else:\n",
    "        date_element = page_soup.find(articles_config[website]['date']['tag_name'], class_=articles_config[website]['date']['class_name'])\n",
    "        if date_element:\n",
    "            date_text_old = date_element.text\n",
    "            regEx = r'(?:\\d{1,2}[-/th|st|nd|rd\\s]*)?(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z\\s,.]*(?:\\d{1,2}[-/th|st|nd|rd)\\s,]*)?(?:\\d{2,4})'\n",
    "            date_text_l = re.findall(regEx, date_text_old)\n",
    "            date_text = date_text_l[0] if date_text_l else \"\"\n",
    "            date = date_text\n",
    "        else:\n",
    "            date = \"\"\n",
    "\n",
    "        class_names = articles_config[website]['context'].get('class_names')\n",
    "        if class_names:\n",
    "            paragraphs = []\n",
    "            strings_to_remove = [\"Related Article:\", \"See related article:\", \"To learn more, visit\"]\n",
    "            for class_name in class_names:\n",
    "                paragraph_elements = page_soup.findAll(articles_config[website]['context']['tag_name'], class_=class_name)\n",
    "                for p in paragraph_elements:\n",
    "                    a_tag_target = p.find('a', href=True, target=\"_blank\", rel=\"noreferrer noopener\")\n",
    "                    if a_tag_target:\n",
    "                        a_tag_target.extract()\n",
    "                        for string_to_remove in strings_to_remove:\n",
    "                            p.string = p.get_text().replace(string_to_remove, \"\").strip()\n",
    "                    em_tags = p.find_all('em')\n",
    "                    a_tag = p.find('a', href=True)\n",
    "                    for em_tag in em_tags:\n",
    "                        if a_tag and em_tag:\n",
    "                            em_tag.extract()\n",
    "                            a_tag.extract()\n",
    "                    strong_tags = p.find_all('strong')\n",
    "                    for strong_tag in strong_tags:\n",
    "                        a_tag_inside_strong = strong_tag.find('a', href=True)\n",
    "                        if a_tag_inside_strong:\n",
    "                            strong_tag.extract()\n",
    "                    paragraphs.append(p.get_text().strip())\n",
    "        else:\n",
    "            paragraphs = []\n",
    "        paragraphs = ' '.join(paragraphs)\n",
    "        if title != extra_info:\n",
    "            paragraphs = extra_info + '\\n' + paragraphs\n",
    "        paragraphs = re.split('\\n+', paragraphs)\n",
    "\n",
    "    if website == \"indiatimes\":\n",
    "        paragraphs = []\n",
    "        for p in page_soup.find_all(['p', 'h4','strong']):\n",
    "        #for p in page_soup.find_all('p'):\n",
    "            if \"Also Read:\" not in p.get_text():\n",
    "                paragraph_text = p.get_text()\n",
    "                #strong_texts = ' '.join(strong.get_text() for strong in p.find_all('strong'))\n",
    "                #if strong_texts:\n",
    "                  #  paragraph_text += ' ' + strong_texts\n",
    "                paragraphs.append(paragraph_text)\n",
    "        #paragraphs = [p.get_text() + ' ' + ' '.join(strong.get_text() for strong in p.find_all('strong')) for p in page_soup.find_all('p') if \"Also Read:\" not in p.get_text()]\n",
    "        \n",
    "          #paragraphs = [p.get_text() for p in page_soup.find_all('p') if \"Also Read:\" not in p.get_text()]\n",
    "\n",
    "    if paragraphs and (\"source:\" in paragraphs[-1].lower() or \"esg clarity’fund\" in paragraphs[-1].lower()):\n",
    "        paragraphs = paragraphs[:-1]\n",
    "\n",
    "    print(\"Date : \", date)\n",
    "    if len(paragraphs) >= 1:\n",
    "        if paragraphs[0] != \"\":\n",
    "            title = title + \".\" if title and not title.endswith(\".\") else title\n",
    "            author = author + \".\" if author and not author.endswith(\".\") else author\n",
    "            date = date + \".\" if date and not date.endswith(\".\") else date\n",
    "            if title:\n",
    "                response_obj['title'] = title\n",
    "            else:\n",
    "                response_obj['title'] = \"\"\n",
    "                response_obj[\"message\"] += \"Title is empty.\"\n",
    "            if author:\n",
    "                response_obj['author'] = author\n",
    "            else:\n",
    "                response_obj['author'] = \"\"\n",
    "                response_obj[\"message\"] += \"Author is empty.\"\n",
    "            if date:\n",
    "                response_obj['date'] = date\n",
    "            else:\n",
    "                response_obj['date'] = \"\"\n",
    "                response_obj[\"message\"] += \"Date is empty.\"\n",
    "            return response_obj, paragraphs\n",
    "        else:\n",
    "            response_obj[\"status\"] = \"error\"\n",
    "            response_obj[\"message\"] = \"No data found.\"\n",
    "            return response_obj, \"\"\n",
    "extractTitleAuthorDateContent(page_soup, articles_config, website)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bcf9771-1749-4b6b-b991-443feff8d5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extractTitleAuthorDateContent(page_soup,articles_config,website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1da81b65-b6eb-4bee-a2ab-256541ed4247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractedArticlesToTextFile(url, page_soup, articles_config, response_obj, user_id, user_type, storage='local'):\n",
    "    '''\n",
    "    Extracting Article Contents and saving as a Text File\n",
    "    '''\n",
    "    # \"domain\" to extract webpage name - used as a \"key\" in articles_config dictionary\n",
    "    domain = tldextract.extract(url).registered_domain\n",
    "    website = domain.split(\".\")[0]\n",
    "    response_obj, paragraphs = extractTitleAuthorDateContent(page_soup, articles_config, website)\n",
    "    \n",
    "    if response_obj[\"status\"] == \"success\":\n",
    "        title = response_obj[\"title\"]\n",
    "        date = response_obj['date']\n",
    "        author = response_obj['author']\n",
    "        if website == \"business-standard\":\n",
    "            content = '\\n\\n'.join(paragraphs)\n",
    "        else:    \n",
    "            content = '\\n\\n'.join(paragraphs[articles_config[website][\"context\"][\"start_index\"]:articles_config[website][\"context\"][\"end_index\"]]) if \"end_index\" in articles_config[website][\"context\"] else '\\n\\n'.join(paragraphs[articles_config[website][\"context\"][\"start_index\"]:])\n",
    "        \n",
    "        words_count = len(nltk.word_tokenize(content))\n",
    "        doc = docx.Document()\n",
    "        doc.add_paragraph(title)\n",
    "        doc.add_paragraph(f\"{words_count} words.\")\n",
    "        doc.add_paragraph(date)\n",
    "        doc.add_paragraph(author)\n",
    "        doc.add_paragraph(content)\n",
    "        \n",
    "        file_name_split = url.split(\"/\")\n",
    "        if file_name_split[-1] != '':\n",
    "            file_name = file_name_split[-1]\n",
    "        else:\n",
    "            file_name = file_name_split[-2]\n",
    "        \n",
    "        author = author.split(\", \")\n",
    "        author = author[0]\n",
    "        file_name = file_name_split[2].replace('www.', '').replace('.', '-') + '-' + ('-'.join(file_name.split('-')[:5])).title() + \".docx\"\n",
    "        \n",
    "        if user_type == 'PRIVATE':\n",
    "            file_name = str(user_id) + '_' + file_name\n",
    "        \n",
    "        if storage == 'local':\n",
    "            # Use the user's home directory to avoid permission issues\n",
    "            home_directory = Path.home()\n",
    "            path = home_directory / \"input_data\"\n",
    "            if not path.exists():\n",
    "                path.mkdir(parents=True, exist_ok=True)\n",
    "            file_path = path / file_name\n",
    "            doc.save(file_path)\n",
    "            size = os.path.getsize(file_path)\n",
    "            size_in_kb = (size // 1024)\n",
    "        else:\n",
    "            file_path = \"input_data/\" + file_name\n",
    "            # size_in_kb = aws.writeIntoBucket(s3_client, bucket_name, file_path, doc, 'doc')\n",
    "        \n",
    "        response_obj[\"file_path\"] = str(file_path)\n",
    "        response_obj[\"file_size_in_kb\"] = size_in_kb\n",
    "    else:\n",
    "        response_obj[\"status\"] = \"error\"\n",
    "        response_obj[\"message\"] = \"No data found in {}\".format(url)\n",
    "    \n",
    "    return response_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "017c6e7e-5b93-46a9-a24c-e64ac24971c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title :  Climeworks Announces Direct Air Capture Technology Breakthrough to Scale Carbon Removal\n",
      "Author :  Susan Lahey\n",
      "Date :  June 5, 2024\n"
     ]
    }
   ],
   "source": [
    "response_obj=extractTitleAuthorDateContent(page_soup,articles_config,website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0dcb13cc-08d8-44e7-99e1-12c4aaf8fcdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title :  Climeworks Announces Direct Air Capture Technology Breakthrough to Scale Carbon Removal\n",
      "Author :  Susan Lahey\n",
      "Date :  June 5, 2024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'message': '',\n",
       " 'file_path': 'C:\\\\Users\\\\admin\\\\input_data\\\\esgtoday-com-Climeworks-Announces-Direct-Air-Capture.docx',\n",
       " 'file_size_in_kb': 37,\n",
       " 'title': 'Climeworks Announces Direct Air Capture Technology Breakthrough to Scale Carbon Removal.',\n",
       " 'author': 'Susan Lahey.',\n",
       " 'date': 'June 5, 2024.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractedArticlesToTextFile(url,page_soup,articles_config,response_obj,user_id,user_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d9cc29a-e3e0-4766-8e79-c89ed7522f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractArticleContentsMain(url,articles_config,user_id,user_type):\n",
    "    '''\n",
    "    Main Function to Get Request from the Webpage using BeautifulSoup and Extracting Contents from Article \n",
    "    '''\n",
    "    response_obj = {\"status\": \"success\", \"message\": \"\", \"file_path\": \"\",\"file_size_in_kb\":\"\"}\n",
    "    try:\n",
    "        # To get request from the wepage url\n",
    "        cookie_jar = http.cookiejar.CookieJar()\n",
    "        opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cookie_jar))\n",
    "        urllib.request.install_opener(opener)\n",
    "        headers = {\n",
    "            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',\n",
    "            'Referer': url\n",
    "        }\n",
    "\n",
    "        req = urllib.request.Request(url, headers=headers)\n",
    "        webpage = urllib.request.urlopen(req).read()\n",
    "        page_soup = soup(webpage, \"html.parser\")\n",
    "        \n",
    "        # Get response_obj dict after writing the contents extracted from the Article webpage\n",
    "        response_obj = extractedArticlesToTextFile(url,page_soup,articles_config,response_obj,user_id,user_type)\n",
    "        return response_obj\n",
    "    except urllib.error.HTTPError as e:\n",
    "        response_obj[\"status\"] = \"error\"\n",
    "        response_obj[\"message\"] = \"Unable to process {}, try after sometime.\".format(url)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(\"Exception occurred extract_article**********\",str(e))\n",
    "        exception_error = ''.join(traceback.TracebackException.from_exception(e).format())\n",
    "        filename = \"extract_article_error_\"+\".txt\"\n",
    "\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, \"a\") as file:\n",
    "                file.write(f\"{exception_error}\\n\")\n",
    "        else:\n",
    "            with open(filename, \"w\") as file:\n",
    "                file.write(f\"{exception_error}\\n\")    \n",
    "        return response_obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ffb9521-edd5-432a-93ad-dc905c0aef97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title :  Climeworks Announces Direct Air Capture Technology Breakthrough to Scale Carbon Removal\n",
      "Author :  Susan Lahey\n",
      "Date :  June 5, 2024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'message': '',\n",
       " 'file_path': 'C:\\\\Users\\\\admin\\\\input_data\\\\esgtoday-com-Climeworks-Announces-Direct-Air-Capture.docx',\n",
       " 'file_size_in_kb': 37,\n",
       " 'title': 'Climeworks Announces Direct Air Capture Technology Breakthrough to Scale Carbon Removal.',\n",
       " 'author': 'Susan Lahey.',\n",
       " 'date': 'June 5, 2024.'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractArticleContentsMain(url,articles_config,user_id,user_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5323eee5-d6c3-4f51-9035-ff0dd99a2e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractMulitLinks(multi_links,user_id,user_type):\n",
    "    with open(\"articles_config.json\") as f:\n",
    "        articles_config = json.load(f)\n",
    "    response_obj = {\"status\": \"success\", \"message\": \"\",  \"files\": []}\n",
    "    \n",
    "    try:\n",
    "        if len(multi_links) >= 1 and multi_links[0] == \"\":\n",
    "            response_obj[\"status\"] = \"error\"\n",
    "            response_obj[\"message\"] = \"No url found\"\n",
    "            return response_obj\n",
    "\n",
    "        elif len(multi_links) > 5:\n",
    "            response_obj[\"status\"] = \"error\"\n",
    "            response_obj[\"message\"] = \"url limit exceeded\"\n",
    "            return response_obj\n",
    "            \n",
    "        for i in multi_links:\n",
    "            domain = tldextract.extract(i).registered_domain\n",
    "            website = domain.split(\".\")[0]\n",
    "            print(\"website : \", website)\n",
    "            if website not in articles_config:\n",
    "                response_obj[\"status\"] = \"error\"\n",
    "                response_obj[\"message\"] = \"Link source is not recognized. Support sites are: esgtoday.com, esgnews.com, esgclarity.com, reuters.com,theconversation.com, business-standard.com\"\n",
    "                return response_obj\n",
    "            \n",
    "            inner_dict = {}\n",
    "            status = extractArticleContentsMain(i,articles_config,user_id,user_type)\n",
    "            response_obj['message']=status['message']\n",
    "            if status[\"status\"] == \"error\":\n",
    "                return status\n",
    "            else:\n",
    "                inner_dict[\"file_path\"] = status[\"file_path\"]\n",
    "                inner_dict[\"file_size_in_kb\"] = status[\"file_size_in_kb\"]\n",
    "                response_obj[\"files\"].append(inner_dict)\n",
    "        return response_obj\n",
    "    except Exception as e:\n",
    "        print(\"Exception occurred extract_article_content**********\",str(e))\n",
    "        exception_error = ''.join(traceback.TracebackException.from_exception(e).format())\n",
    "        print(exception_error)\n",
    "        response_object[\"message\"] = \"unable to process : {}\".format(i) \n",
    "        return response_object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f863ad2b-db17-411d-b220-82b7cddc13a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_links=['https://esgnews.com/china-dominates-global-wind-and-solar-energy-construction/','https://theconversation.com/young-people-led-surge-for-smaller-parties-but-no-reform-youthquake-says-uk-election-survey-234394','https://www.business-standard.com/india-news/eighty-million-new-jobs-in-last-three-to-four-years-says-pm-modi-124071300691_1.html','https://www.indiatimes.com/lifestyle/whats-cooking/ambani-wedding-kim-kardashian-khloe-kardashian-look-638072.html','https://www.indiatimes.com/health/healthyliving/why-you-should-stop-walking-barefoot-at-home-636547.html']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acaa220f-9e26-47ae-b966-73e2e6380127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "website :  esgnews\n",
      "Title :  China Dominates Global Wind and Solar Energy Construction\n",
      "Author :  ESG News\n",
      "Date :  July 12, 2024\n",
      "website :  theconversation\n",
      "Title :  Young people led surge for smaller parties but no Reform ‘youthquake’, says UK election survey\n",
      "Author :  Stuart Fox\n",
      "Date :  July 12, 2024\n",
      "website :  business-standard\n",
      "Title :  Eighty million new jobs in last three to four years, says PM Modi\n",
      "Author :  Press Trust of India\n",
      "Date :  Jul 13 2024\n",
      "website :  indiatimes\n",
      "Title :  Ambani wedding: Check out Kim K and Khloé Kardashian's desi Barbie looks, Internet is not feeling it\n",
      "Author :  IT Lifestyle Desk\n",
      "Date :  Jul 13, 2024\n",
      "website :  indiatimes\n",
      "Title :  6 reasons why you should stop walking barefoot at home right now\n",
      "Author :  IT Lifestyle Desk\n",
      "Date :  Jul 13, 2024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'message': '',\n",
       " 'files': [{'file_path': 'C:\\\\Users\\\\admin\\\\input_data\\\\esgnews-com-China-Dominates-Global-Wind-And.docx',\n",
       "   'file_size_in_kb': 37},\n",
       "  {'file_path': 'C:\\\\Users\\\\admin\\\\input_data\\\\theconversation-com-Young-People-Led-Surge-For.docx',\n",
       "   'file_size_in_kb': 38},\n",
       "  {'file_path': 'C:\\\\Users\\\\admin\\\\input_data\\\\business-standard-com-Eighty-Million-New-Jobs-In.docx',\n",
       "   'file_size_in_kb': 35},\n",
       "  {'file_path': 'C:\\\\Users\\\\admin\\\\input_data\\\\indiatimes-com-Ambani-Wedding-Kim-Kardashian-Khloe.docx',\n",
       "   'file_size_in_kb': 37},\n",
       "  {'file_path': 'C:\\\\Users\\\\admin\\\\input_data\\\\indiatimes-com-Why-You-Should-Stop-Walking.docx',\n",
       "   'file_size_in_kb': 37}]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractMulitLinks(multi_links,user_id,user_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "accaadf7-9324-4139-a860-5a5c90549e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_present_in_db(url):\n",
    "    file_name_split = url.split(\"/\")\n",
    "    if file_name_split[-1]!='':\n",
    "        file_name = file_name_split[-1]\n",
    "    else:\n",
    "        file_name = file_name_split[-2]\n",
    "    file_name = file_name_split[2].replace('www.','').replace('.','-')+'-'+('-'.join(file_name.split('-')[:5])).title()  + \".docx\"\n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bbbd205-84ea-482f-9f5c-3a2b1943ebe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'esgtoday-com-Climeworks-Announces-Direct-Air-Capture.docx'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_present_in_db(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c86bcd22-25bc-4e1f-97e7-5d622eda5e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkDocProcessed_links(url,user_id,user_type):\n",
    "    file_name = file_present_in_db(url)\n",
    "    if user_type == 'PRIVATE':\n",
    "        file_name = str(user_id)+'_'+file_name\n",
    "    #query = \"select * from playground_doc_cache_info where doc_name = '\" + file_name + \"'\"\n",
    "    #sql_df = util.loadDataFromSqlToDf(serverConfig[\"mysql_config\"], query)\n",
    "    #if file_name in sql_df[\"doc_name\"].tolist():\n",
    "       # return file_name\n",
    "    #else:\n",
    "        #return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "298b819c-fbbb-4f78-bfa2-42a84f09c028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_links(multi_links,user_id,user_type):\n",
    "    response_obj = {\"status\": \"success\", \"message\": \"\", \"files\": []}\n",
    "    for url in multi_links:\n",
    "        # file_name = file_present_in_db(url)\n",
    "        result = checkDocProcessed_links(url,user_id,user_type)\n",
    "        if result is None:\n",
    "            resp_object = extractMulitLinks([url],user_id,user_type) \n",
    "            response_obj[\"files\"].append(resp_object[\"files\"][0])\n",
    "        else:\n",
    "            inner_dict = {}\n",
    "            file_name = file_present_in_db(url)\n",
    "            if storage == 'local':\n",
    "                p = Path(str(os.path.abspath(os.curdir)))\n",
    "                path = p.parent\n",
    "                file_path = str(path)+\"/input_data/\"+file_name\n",
    "                size = os.path.getsize(file_path)\n",
    "                size_in_kb = size // 1024\n",
    "            else:\n",
    "                file_path = \"input_data/\"+file_name\n",
    "                size_in_kb = aws.getS3BucketFileSize(aws.S3_Client,bucket_name, file_path)\n",
    "            inner_dict[\"file_path\"] = file_name\n",
    "            inner_dict[\"file_size_in_kb\"] = size_in_kb\n",
    "            response_obj[\"files\"].append(inner_dict)\n",
    "    return response_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6cf3ebb7-5d98-4a86-9fad-3ab3b855f60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "website :  esgnews\n",
      "Title :  China Dominates Global Wind and Solar Energy Construction\n",
      "Author :  ESG News\n",
      "Date :  July 12, 2024\n",
      "website :  theconversation\n",
      "Title :  Young people led surge for smaller parties but no Reform ‘youthquake’, says UK election survey\n",
      "Author :  Stuart Fox\n",
      "Date :  July 12, 2024\n",
      "website :  business-standard\n",
      "Title :  Eighty million new jobs in last three to four years, says PM Modi\n",
      "Author :  Press Trust of India\n",
      "Date :  Jul 13 2024\n",
      "website :  indiatimes\n",
      "Title :  Ambani wedding: Check out Kim K and Khloé Kardashian's desi Barbie looks, Internet is not feeling it\n",
      "Author :  IT Lifestyle Desk\n",
      "Date :  Jul 13, 2024\n",
      "website :  indiatimes\n",
      "Title :  6 reasons why you should stop walking barefoot at home right now\n",
      "Author :  IT Lifestyle Desk\n",
      "Date :  Jul 13, 2024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'message': '',\n",
       " 'files': [{'file_path': 'C:\\\\Users\\\\admin\\\\input_data\\\\esgnews-com-China-Dominates-Global-Wind-And.docx',\n",
       "   'file_size_in_kb': 37},\n",
       "  {'file_path': 'C:\\\\Users\\\\admin\\\\input_data\\\\theconversation-com-Young-People-Led-Surge-For.docx',\n",
       "   'file_size_in_kb': 38},\n",
       "  {'file_path': 'C:\\\\Users\\\\admin\\\\input_data\\\\business-standard-com-Eighty-Million-New-Jobs-In.docx',\n",
       "   'file_size_in_kb': 35},\n",
       "  {'file_path': 'C:\\\\Users\\\\admin\\\\input_data\\\\indiatimes-com-Ambani-Wedding-Kim-Kardashian-Khloe.docx',\n",
       "   'file_size_in_kb': 37},\n",
       "  {'file_path': 'C:\\\\Users\\\\admin\\\\input_data\\\\indiatimes-com-Why-You-Should-Stop-Walking.docx',\n",
       "   'file_size_in_kb': 37}]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_links(multi_links,user_id,user_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5580a915-e0ab-4372-baae-2d65dcb0aba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'esgtoday': {'title': {'tag_name': 'h1',\n",
       "   'class_name': 'post-title entry-title'},\n",
       "  'author': {'tag_name': 'span', 'class_name': 'author vcard'},\n",
       "  'date': {'tag_name': 'time', 'class_name': 'post-date entry-date updated'},\n",
       "  'context': {'tag_name': 'div',\n",
       "   'class_names': ['entry-content'],\n",
       "   'start_index': 0}},\n",
       " 'esgnews': {'title': {'tag_name': 'h1', 'class_name': 'c-h1 tw-mb-[0.5em]'},\n",
       "  'author': {'tag_name': 'a', 'class_name': 'tw-text-black hover:tw-text-red'},\n",
       "  'date': {'tag_name': 'div', 'class_name': 'tw-p-2'},\n",
       "  'context': {'tag_name': 'div',\n",
       "   'class_names': ['simple-text tt-content title-droid margin-big tw-text-base tw-leading-relaxed'],\n",
       "   'start_index': 1}},\n",
       " 'theconversation': {'title': {'tag_name': 'h1',\n",
       "   'class_name': 'legacy entry-title instapaper_title'},\n",
       "  'author': {'tag_name_1': 'span',\n",
       "   'class_name_1': 'fn author-name',\n",
       "   'tag_name_2': 'div',\n",
       "   'class_name_2': 'content-authors-group',\n",
       "   'tag_name_3': 'h3',\n",
       "   'text_list': ['Author', 'Authors']},\n",
       "  'date': {'tag_name': 'div', 'class_name': 'timestamps'},\n",
       "  'context': {'tag_name': 'div',\n",
       "   'class_names': ['grid-ten large-grid-nine grid-last content-body content entry-content instapaper_body inline-promos',\n",
       "    'grid-ten large-grid-nine grid-last content-body content entry-content instapaper_body'],\n",
       "   'start_index': 0}},\n",
       " 'esgclarity': {'title': {'tag_name': 'div', 'class_name': 'content-section'},\n",
       "  'author': {'tag_name': 'span', 'class_name': 'author vcard'},\n",
       "  'date': {'tag_name': 'time', 'class_name': 'entry-date published updated'},\n",
       "  'context': {'tag_name': 'div',\n",
       "   'class_names': ['entry-content'],\n",
       "   'start_index': 0}},\n",
       " 'reuters': {'title': {'tag_name': 'h1',\n",
       "   'class_name': 'text__text__1FZLe text__dark-grey__3Ml43 text__medium__1kbOh text__heading_3__1kDhc heading__base__2T28j heading__heading_3__3aL54 article-header__title__3Y2hh'},\n",
       "  'author': {'tag_name': 'a', 'class_name': 'author-name__author__1gx5k'},\n",
       "  'date': {'tag_name': 'span', 'class_name': 'date-line__date__23Ge-'},\n",
       "  'context': {'start_index': 0, 'end_index': -11}},\n",
       " 'business-standard': {'title': {'tag_name': 'h1', 'class_name': 'stryhdtp'},\n",
       "  'extra': {'tag_name': 'h2', 'class_name': 'strydsc'},\n",
       "  'author': {'tag_name': 'span', 'class_name': 'font-bold'},\n",
       "  'date': {'tag_name': 'div', 'class_name': 'meta-info'},\n",
       "  'context': {'tag_name': 'div', 'class_names': ['storycontent']}},\n",
       " 'indiatimes': {'title': {'tag_name': 'h1', 'class_name': 'article-title'},\n",
       "  'author': {'tag_name': 'a', 'class_name': 'author-strip-editor font-14'},\n",
       "  'date': {'tag_name': 'div', 'class_name': 'author-strip-date font-11'},\n",
       "  'context': {'tag_name': 'div',\n",
       "   'class_names': ['article-description-0'],\n",
       "   'start_index': 0}}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e682fc28-c834-49a5-be07-323cb0f8ba0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36931169-39de-4979-bf5e-3f3a86334bc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7379b1-0e1a-4f7f-a026-ac527ace53df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7a7b83-1649-42ff-8abc-1394e3fd595a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
